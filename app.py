import streamlit as st
import pandas as pd
import numpy as np
import os
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

# ä»¥ä¸‹å¥—ä»¶ç‚ºå¯é¸ï¼Œè‹¥æœªå®‰è£å‰‡é™ç´šé¡¯ç¤º
try:
    import seaborn as sns
except Exception:
    sns = None
try:
    import plotly.express as px
except Exception:
    px = None
try:
    from wordcloud import WordCloud
except Exception:
    WordCloud = None
try:
    import jieba
except Exception:
    jieba = None

@st.cache_data
def load_data():
    # å„ªå…ˆè¼‰å…¥ spam.csvï¼Œå†è©¦ test_spam.csvï¼Œæœ€å¾Œå…§å»ºç¤ºä¾‹
    paths = ['spam.csv', 'test_spam.csv']
    for p in paths:
        if os.path.exists(p):
            try:
                df = pd.read_csv(p, encoding='latin-1', low_memory=False)
                if {'v1','v2'}.issubset(df.columns):
                    df = df[['v1','v2']].dropna(subset=['v1','v2'])
                    df.columns = ['label','text']
                    df['text'] = df['text'].astype(str)
                    return df
            except Exception:
                continue
    # å…§å»ºç¤ºä¾‹ï¼ˆæœ€å°å¯ç”¨é›†ï¼‰
    sample = pd.DataFrame({
        'label': ['ham','spam','ham','spam','ham','spam'],
        'text': [
            'æ˜å¤©ä¸‹åˆ 2 é»é–‹æœƒï¼Œè«‹æº–æ™‚ã€‚',
            'æ­å–œä½ ï¼ä¸­çäº†ï¼Œé»æ“Šé ˜å–çå“ã€‚',
            'è«‹å¹«æˆ‘ç¢ºèªé™„ä»¶æ–‡ä»¶å…§å®¹ã€‚',
            'é™æ™‚å„ªæƒ  50% æŠ˜æ‰£ï¼Œç«‹å³æ¶è³¼ï¼',
            'åˆé¤è¦è¨‚ä»€éº¼ï¼Ÿ',
            'æ‚¨è¢«é¸ä¸­ç²å¾—å…è²»æ—…éŠï¼Œé»æ­¤ç”³è«‹ã€‚'
        ]
    })
    return sample

def plot_confusion(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(5,4))
    if sns:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    xticklabels=['ham','spam'], yticklabels=['ham','spam'])
    else:
        ax.imshow(cm, cmap='Blues')
        for (i,j),v in np.ndenumerate(cm):
            ax.text(j, i, str(v), ha='center', va='center', color='white')
        ax.set_xticks([0,1]); ax.set_yticks([0,1])
        ax.set_xticklabels(['ham','spam']); ax.set_yticklabels(['ham','spam'])
    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual'); ax.set_title('Confusion Matrix')
    return fig

def make_wordcloud(text):
    if WordCloud is None:
        return None
    text_joined = ' '.join(text)
    font_path = None
    if os.name == 'nt':
        candidate = r"C:\Windows\Fonts\msyh.ttc"
        if os.path.exists(candidate):
            font_path = candidate
    try:
        if font_path:
            wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path).generate(text_joined)
        else:
            wc = WordCloud(width=800, height=400, background_color='white').generate(text_joined)
        return wc
    except Exception:
        return None

def safe_report_dict(y_test, y_pred):
    try:
        return classification_report(y_test, y_pred, output_dict=True)
    except Exception:
        return {}

def main():
    st.set_page_config(page_title="åƒåœ¾éƒµä»¶åˆ†é¡å™¨", layout="wide")
    st.title("ğŸ“§ æ™ºæ…§åƒåœ¾éƒµä»¶åˆ†é¡å™¨")
    st.write("ç³»çµ±æœƒè‡ªå‹•è¼‰å…¥ spam.csv æˆ– test_spam.csvï¼Œè‹¥ç„¡å‰‡ä½¿ç”¨å…§å»ºç¤ºä¾‹ã€‚")

    data = load_data()
    if data is None or data.empty:
        st.error("ç„¡å¯ç”¨è³‡æ–™")
        st.stop()

    # å´é‚Šæ¬„ï¼šæ¨¡å‹è¨­å®šèˆ‡è³‡æ–™çµ±è¨ˆ
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        min_df = st.slider("Tfidf min_df", 1, 5, 1)
        test_size = st.slider("æ¸¬è©¦é›†æ¯”ä¾‹", 0.1, 0.4, 0.2)
        st.markdown("---")
        st.subheader("è³‡æ–™çµ±è¨ˆ")
        st.write(f"è³‡æ–™ç­†æ•¸ï¼š{len(data)}")
        spam_count = int((data['label']=='spam').sum())
        st.write(f"åƒåœ¾éƒµä»¶æ•¸ï¼š{spam_count}  ({spam_count/len(data):.1%})")
        st.markdown("---")
        st.write("é—œæ–¼ï¼šç‰ˆæœ¬ 1.0")

    # å‰è™•ç†èˆ‡è¨“ç·´
    X_text = data['text'].astype(str)
    y = (data['label']=='spam').astype(int)
    vectorizer = TfidfVectorizer(min_df=min_df)
    try:
        X = vectorizer.fit_transform(X_text)
    except Exception as e:
        st.error(f"å‘é‡åŒ–å¤±æ•—ï¼š{e}")
        st.stop()

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    model = MultinomialNB()
    model.fit(X_train, y_train)

    # è©•ä¼°
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    report = safe_report_dict(y_test, y_pred)

    st.markdown("## æ¨¡å‹æ‘˜è¦")
    c1, c2, c3 = st.columns(3)
    c1.metric("æº–ç¢ºç‡", f"{acc:.2%}")
    prec = report.get('1', {}).get('precision') if isinstance(report, dict) else report.get('spam',{}).get('precision',0.0)
    rec = report.get('1', {}).get('recall') if isinstance(report, dict) else report.get('spam',{}).get('recall',0.0)
    c2.metric("åƒåœ¾éƒµä»¶ç²¾ç¢ºåº¦", f"{(prec or 0):.2%}")
    c3.metric("åƒåœ¾éƒµä»¶å¬å›ç‡", f"{(rec or 0):.2%}")

    # ä¸»å€ï¼šè¼¸å…¥èˆ‡é æ¸¬
    st.markdown("---")
    st.subheader("ğŸ“ éƒµä»¶åˆ†æ")
    user_text = st.text_area("è²¼ä¸Šéƒµä»¶å…§å®¹ï¼ŒæŒ‰ã€Œåˆ†æã€ï¼š", height=180)
    if st.button("åˆ†æ"):
        if not user_text.strip():
            st.warning("è«‹è¼¸å…¥å…§å®¹")
        else:
            with st.spinner("åˆ†æä¸­..."):
                time.sleep(0.5)
                vec = vectorizer.transform([user_text])
                pred = model.predict(vec)[0]
                proba = model.predict_proba(vec)[0]
                if pred == 1:
                    st.error("âš ï¸ å¯èƒ½æ˜¯åƒåœ¾éƒµä»¶")
                else:
                    st.success("âœ… å¯èƒ½æ˜¯æ­£å¸¸éƒµä»¶")
                # é¡¯ç¤ºæ©Ÿç‡
                labels = ['æ­£å¸¸','åƒåœ¾']
                probs = [proba[0], proba[1]]
                if px:
                    dfp = pd.DataFrame({'é¡åˆ¥':labels,'æ©Ÿç‡':probs})
                    figp = px.bar(dfp, x='é¡åˆ¥', y='æ©Ÿç‡', color='é¡åˆ¥',
                                  color_discrete_map={'æ­£å¸¸':'green','åƒåœ¾':'red'}, range_y=[0,1])
                    st.plotly_chart(figp, use_container_width=True)
                else:
                    fig, ax = plt.subplots()
                    ax.bar(labels, probs, color=['green','red'])
                    ax.set_ylim(0,1)
                    st.pyplot(fig)
                # é€²éšï¼šæ–‡å­—é›²ï¼ˆè‹¥å¯ï¼‰
                st.markdown("### ğŸ” é€²éšåˆ†æ")
                if WordCloud is not None:
                    tokens = jieba.lcut(user_text) if jieba else user_text.split()
                    wc = make_wordcloud(tokens)
                    if wc:
                        fig_wc = plt.figure(figsize=(8,3))
                        plt.imshow(wc, interpolation='bilinear')
                        plt.axis('off')
                        st.pyplot(fig_wc)
                        plt.close()
                    else:
                        st.info("ç„¡æ³•ç”¢ç”Ÿæ–‡å­—é›²ï¼ˆç¼ºå°‘è³‡æºæˆ–å­—å‹ï¼‰")
                else:
                    st.info("æœªå®‰è£ wordcloudï¼Œç„¡æ³•é¡¯ç¤ºæ–‡å­—é›²ã€‚")

    # æ¨¡å‹è©•ä¼°è©³ç´°
    st.markdown("---")
    st.subheader("ğŸ“Š æ¨¡å‹è©•ä¼°")
    cm_fig = plot_confusion(y_test, y_pred)
    st.pyplot(cm_fig)
    st.code(classification_report(y_test, y_pred, target_names=['ham','spam']))

if __name__ == "__main__":
    main()
